{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's start by importing the most important libraries. We will primarily use Pandas and Numpy\\\n",
    "#, some modules from NLTK, and re for regular expressions\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import re;\n",
    "import time;\n",
    "from nltk.corpus import stopwords;\n",
    "import json;\n",
    "import sys;\n",
    "import pickle;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fill in the data location, which has been obtained from https://www.kaggle.com/bittlingmayer/amazonreviews/data\n",
    "\n",
    "data_location = 'input/amazon_review_full_csv/';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load in the train and test sets. No header.\n",
    "data_train = pd.read_csv(data_location + 'train.csv', header=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(data_location + 'test.csv', header=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Update the headers with the right column names\n",
    "data_train.columns = ['rating', 'subject', 'review'];\n",
    "data_test.columns = ['rating', 'subject', 'review'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000\n",
      "650000\n"
     ]
    }
   ],
   "source": [
    "print(len(data_train));\n",
    "print(len(data_test));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How much of the data we are using: 4.5%\n"
     ]
    }
   ],
   "source": [
    "#There are a lot of rows in both the train and test sets, so to make processing easier, we will \n",
    "#use a small subset of the rows to verify that the pipeline works without errors.\n",
    "\n",
    "#We can do the final training on an AWS server.\n",
    "\n",
    "#First, we need to get a subset of indices. The sampling dataset\n",
    "#will be divided into the following ratio: 75% TRAIN, 20% DEV, 5% TEST\n",
    "\n",
    "np.random.seed(1024);\n",
    "\n",
    "total_samples = int(0.06 * len(data_train))\n",
    "rand_indices = np.random.choice(len(data_train), total_samples, replace=False);\n",
    "\n",
    "train_split_index = int(0.75 * total_samples);\n",
    "dev_split_index   = int(0.95 * total_samples);\n",
    "\n",
    "data_sample_train = data_train.iloc[rand_indices[:train_split_index]];\n",
    "data_sample_dev   = data_train.iloc[rand_indices[train_split_index:dev_split_index]];\n",
    "data_sample_test  = data_train.iloc[rand_indices[dev_split_index:]];\n",
    "\n",
    "sample_ratio = len(data_train) / len(data_sample_train);\n",
    "print(\"How much of the data we are using: \" + str(100.0 / sample_ratio) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Next step is cleaning. We will go through different steps, and in the end combine them into a single function. Because we\n",
    "#are only writing the functions to build the pipeline, I will use a small subset of the data sample to verify it.\n",
    "\n",
    "data_subsample = data_sample_train.iloc[0:100];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 22.0% of rows\n"
     ]
    }
   ],
   "source": [
    "#First step: Remove all items that have a Neutral (3 star) rating\n",
    "\n",
    "data_sub_filtered = data_subsample[data_subsample.rating != 3];\n",
    "rows_removed = len(data_subsample) - len(data_sub_filtered);\n",
    "\n",
    "print('Removing ' + str(100.0 * rows_removed / len(data_subsample)) + '% of rows');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chawlar\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py36\\lib\\site-packages\\pandas\\core\\indexing.py:517: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "#Second step: Binary ratings. 0 for {1, 2} and 1 for {4, 5}\n",
    "\n",
    "data_sub_filtered.loc[data_sub_filtered.rating <= 2, 'rating'] = 0;\n",
    "data_sub_filtered.loc[data_sub_filtered.rating >= 4, 'rating'] = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0.0% of rows\n"
     ]
    }
   ],
   "source": [
    "#Third step: Remove all NaNs.\n",
    "#We have enough rows in our dataset that we can safely remove all rows with NaNs and still have enough data.\n",
    "\n",
    "rows_before = len(data_sub_filtered);\n",
    "data_sub_filtered = data_sub_filtered.dropna();\n",
    "print('Removed ' + str(100*(len(data_sub_filtered)-rows_before) / len(data_sub_filtered)) + '% of rows');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>subject</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>893521</th>\n",
       "      <td>0</td>\n",
       "      <td>Lousy</td>\n",
       "      <td>This is a lousy movie and does not follow the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2810525</th>\n",
       "      <td>0</td>\n",
       "      <td>failed on induction cooktop</td>\n",
       "      <td>I bought it to use on my new Kitchenaid induct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363307</th>\n",
       "      <td>1</td>\n",
       "      <td>Heart Warming</td>\n",
       "      <td>This was a great movie for the whole family. E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999390</th>\n",
       "      <td>0</td>\n",
       "      <td>Maddening.</td>\n",
       "      <td>About the region problem, the best solution is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2325074</th>\n",
       "      <td>1</td>\n",
       "      <td>A great soundtrack!</td>\n",
       "      <td>the waynes world soundtrack is hella tight! th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         rating                      subject  \\\n",
       "893521        0                        Lousy   \n",
       "2810525       0  failed on induction cooktop   \n",
       "1363307       1                Heart Warming   \n",
       "2999390       0                   Maddening.   \n",
       "2325074       1          A great soundtrack!   \n",
       "\n",
       "                                                    review  \n",
       "893521   This is a lousy movie and does not follow the ...  \n",
       "2810525  I bought it to use on my new Kitchenaid induct...  \n",
       "1363307  This was a great movie for the whole family. E...  \n",
       "2999390  About the region problem, the best solution is...  \n",
       "2325074  the waynes world soundtrack is hella tight! th...  "
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I don't understand the warning above, but it looks like the data was properly processed\n",
    "#Llet's look at a few rows of the data so far\n",
    "data_sub_filtered.iloc[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 0.07255768775939941s\n"
     ]
    }
   ],
   "source": [
    "#Fourth step: Remove all symbols such as ! % & * @ #\n",
    "\n",
    "#To do this, I will use a regular expression that only keeps alphanumerics and punctuation symbols.\n",
    "pattern_to_find = \"[^a-zA-Z0-9' \\.,!\\?]\";\n",
    "pattern_to_repl = \"\";\n",
    "\n",
    "start = time.time();\n",
    "\n",
    "for row in data_sub_filtered.index:\n",
    "    data_sub_filtered.loc[row, 'subject'] = re.sub(pattern_to_find, pattern_to_repl, data_sub_filtered.loc[row, 'subject']).lower();\n",
    "    \n",
    "    data_sub_filtered.loc[row, 'review'] = re.sub(pattern_to_find, pattern_to_repl, data_sub_filtered.loc[row, 'review']).lower();\n",
    "    \n",
    "print('Total time taken: ' + str(time.time() - start) + 's');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 0.07422900199890137s\n"
     ]
    }
   ],
   "source": [
    "#Fifth step: Remove alll urls from the text\n",
    "\n",
    "#ref: https://stackoverflow.com/a/6883094/1843486\n",
    "\n",
    "start = time.time();\n",
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+';\n",
    "\n",
    "for row in data_sub_filtered.index:\n",
    "\n",
    "    data_sub_filtered.loc[row, 'subject'] = re.sub(url_regex, '', data_sub_filtered.loc[row, 'subject']);\n",
    "    data_sub_filtered.loc[row, 'review'] = re.sub(url_regex, '', data_sub_filtered.loc[row, 'review']);\n",
    "    \n",
    "print('Total time taken: ' + str(time.time() - start) + 's');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ain't  ->  is not\n",
      "aren't  ->  are not\n",
      "can't  ->  cannot\n",
      "can't've  ->  cannot have\n",
      "'cause  ->  because\n",
      "could've  ->  could have\n",
      "couldn't  ->  could not\n"
     ]
    }
   ],
   "source": [
    "#Sixth step: Expand all contractions. This will reduce disambiguation between similar phrases such as I'll and I will.\n",
    "#ref: https://stackoverflow.com/a/19794953/1843486\n",
    "\n",
    "#I placed all contraction mappings in a text json file, which we can load in\n",
    "#and look at a few values to see what we have\n",
    "contractions = json.load(open('contractions.json', 'r'));\n",
    "for pos, key in enumerate(contractions.keys()):\n",
    "    print(key, ' -> ', contractions[key]);\n",
    "    \n",
    "    if pos > 5:\n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 0.07723617553710938s\n"
     ]
    }
   ],
   "source": [
    "#This is a little tricky, and not completely perfect. There are two types of contractions I will be looking at\n",
    "#of the form < is'nt > and of the form < apple's >. The first can be mapped from the json dict,\n",
    "#but the second form cannot because apple is a proper noun. For all instances of the second, I will just expand it to < apple is >\n",
    "start = time.time();\n",
    "\n",
    "apos_regex = \"'[a-z]+|[a-z]+'[a-z]+|[a-z]+'\";\n",
    "\n",
    "#A function allows us to iterate over quickly rather than having a complicated lambda expression\n",
    "def expand(word):\n",
    "    if \"'\" in word:\n",
    "        if word in contractions:\n",
    "            return contractions[word]\n",
    "        if word.endswith(\"'s\"):\n",
    "            return word[:-2] + \" is\"\n",
    "        else:\n",
    "            return word;\n",
    "    else:\n",
    "        return word;\n",
    "\n",
    "for row in data_sub_filtered.index:\n",
    "    data_sub_filtered.loc[row, 'subject'] = ' '.join(([expand(word) for word in data_sub_filtered.loc[row, 'subject'].split()]));\n",
    "    \n",
    "    data_sub_filtered.loc[row, 'review'] =  ' '.join(([expand(word) for word in data_sub_filtered.loc[row,  'review'].split()]));\n",
    "\n",
    "\n",
    "print('Total time taken: ' + str(time.time() - start) + 's');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 0.07076811790466309s\n"
     ]
    }
   ],
   "source": [
    "#Seventh step: Remove all stopwords such as and but if so then\n",
    "\n",
    "eng_stopwords = set(stopwords.words(\"english\"));\n",
    "\n",
    "start = time.time();\n",
    "\n",
    "for row in data_sub_filtered.index:\n",
    "    text_subj = data_sub_filtered.loc[row, 'subject'].split();\n",
    "    text_revi = data_sub_filtered.loc[row, 'review'].split();\n",
    "    \n",
    "    data_sub_filtered.loc[row, 'subject'] = ' '.join([word for word in text_subj if word not in eng_stopwords]);\n",
    "    data_sub_filtered.loc[row, 'review']  = ' '.join([word for word in text_revi if word not in eng_stopwords]);\n",
    "\n",
    "print('Total time taken: ' + str(time.time() - start) + 's');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now that all of the different processes have been completed, it is time to combine them all into a single function.\n",
    "\n",
    "#The process_sentence method will process a single sentence \\\n",
    "#and return a version without URLs, symbols, contractions and stopwords.\n",
    "def process_sentence(sentence):\n",
    "    \n",
    "    #Remove special symbols and lowercase everything\n",
    "    alphanum = re.sub(pattern_to_find, pattern_to_repl, sentence).lower();\n",
    "    \n",
    "    #Remove URLs\n",
    "    nourls = re.sub(url_regex, '', alphanum);\n",
    "    \n",
    "    #Expand all contractions\n",
    "    noapos = ' '.join(([expand(word) for word in nourls.split()]));\n",
    "    \n",
    "    #Remove stopwords\n",
    "    bigwords = ' '.join([word for word in noapos.split() if word not in eng_stopwords]);\n",
    "    \n",
    "    return bigwords;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The clean_data method will take in a dataframe, filter out neutral rows, binarize ratings, remove na's, and process rows.\n",
    "def clean_data(dframe):\n",
    "    start = time.time();\n",
    "    \n",
    "    dframe = dframe[dframe.rating != 3]\n",
    "    \n",
    "    dframe.loc[dframe.rating <= 2, 'rating'] = 0;\n",
    "    dframe.loc[dframe.rating >= 4, 'rating'] = 1;\n",
    "\n",
    "    dframe = dframe.dropna();\n",
    "    \n",
    "    for pos, row in enumerate(dframe.index):\n",
    "        dframe.loc[row, 'subject'] = process_sentence(dframe.loc[row, 'subject']);\n",
    "\n",
    "        dframe.loc[row,  'review'] = process_sentence(dframe.loc[row,  'review']);\n",
    "        \n",
    "        if pos % 1000 == 0 and pos > 0:\n",
    "            time_so_far = (time.time() - start)/60;\n",
    "            time_eta = time_so_far * (len(dframe) / pos) - time_so_far;\n",
    "            sys.stdout.write(\"\\rCompleted \" + str(pos) + \" / \" + str(len(dframe)) + \" in \" + str(time_so_far) + \"m eta: \" + str(time_eta) + 'm');\n",
    "           \n",
    "    print('\\n')\n",
    "    print('Total time taken: ' + str(time.time() - start) + 's');\n",
    "    \n",
    "    return dframe;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Completed 0 / 78 in 0.002055493990580241mTotal time taken: 0.19357585906982422s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chawlar\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py36\\lib\\site-packages\\pandas\\core\\indexing.py:517: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "data_sub_processed = clean_data(data_subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "data_sub_processed.equals(data_sub_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 107000 / 107991 in 35.250710145632425m eta: 0.32648087620862043m\n",
      "\n",
      "\n",
      "Total time taken: 2127.7107167243958s\n"
     ]
    }
   ],
   "source": [
    "data_train_processed = clean_data(data_sample_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chawlar\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py36\\lib\\site-packages\\pandas\\core\\indexing.py:517: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 28000 / 28859 in 2.6152098536491395m eta: 0.0802309022958787mm\n",
      "\n",
      "Total time taken: 159.50025272369385s\n"
     ]
    }
   ],
   "source": [
    "data_dev_processed = clean_data(data_sample_dev);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chawlar\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py36\\lib\\site-packages\\pandas\\core\\indexing.py:517: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 7000 / 7150 in 0.20186723868052164m eta: 0.004325726543154024m\n",
      "\n",
      "Total time taken: 12.259939908981323s\n"
     ]
    }
   ],
   "source": [
    "data_test_processed = clean_data(data_sample_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data_train_processed, open(\"picklefiles/data_train_processed.pkl\", 'wb'));\n",
    "pickle.dump(data_train_processed, open(\"picklefiles/data_dev_processed.pkl\", 'wb'))\n",
    "pickle.dump(data_train_processed, open(\"picklefiles/data_test_processed.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now that we have our data, the next step is to obtain features. I will be training a \n",
    "#convolutional neural network, for which we need vectorized input. One way to do this\n",
    "#is to define each word with a Word2Vec model.\n",
    "\n",
    "#If you read my previous post on Word2Vec, the model is a shallow neural network trained\n",
    "#to learn word embeddings. We will be learning features with the Word2Vec model first,\n",
    "#and then using the features generated from it as inputs to the convolutional neural network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
